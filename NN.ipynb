{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4yHFUeXFL/CRnbWemqKJ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlowSight/MlAlgoFromScratch/blob/master/NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbgEAIfMqRCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import progressbar\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwO-1vJzWost",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid():\n",
        "    def __call__(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def gradient(self, x):\n",
        "        return self.__call__(x) * (1 - self.__call__(x))\n",
        "\n",
        "class Softmax():\n",
        "    def __call__(self, x):\n",
        "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "    def gradient(self, x):\n",
        "        p = self.__call__(x)\n",
        "        return p * (1 - p)\n",
        "\n",
        "class Loss(object):\n",
        "    def loss(self, y_true, y_pred):\n",
        "        return NotImplementedError()\n",
        "\n",
        "    def gradient(self, y, y_pred):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def acc(self, y, y_pred):\n",
        "        return 0\n",
        "\n",
        "class SquareLoss(Loss):\n",
        "    def __init__(self): pass\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        return 0.5 * np.power((y - y_pred), 2)\n",
        "\n",
        "    def gradient(self, y, y_pred):\n",
        "        return -(y - y_pred)\n",
        "\n",
        "    def hessian(self, y, y_pred):\n",
        "        return 1\n",
        "\n",
        "\n",
        "class CrossEntropy(Loss):\n",
        "    def __init__(self): pass\n",
        "\n",
        "    def loss(self, y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
        "\n",
        "    def acc(self, y, p):\n",
        "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
        "\n",
        "    def gradient(self, y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - (y / p) + (1 - y) / (1 - p)\n",
        "\n",
        "    def hessian(self,y,p):\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return y/(p**2) + (1-y)/(1-p)**2\n",
        "\n",
        "class LogisticLoss():\n",
        "    def __init__(self):\n",
        "        sigmoid = Sigmoid()\n",
        "        self.log_func = sigmoid\n",
        "        self.log_grad = sigmoid.gradient\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        p = self.log_func(y_pred)\n",
        "        return y * np.log(p) + (1 - y) * np.log(1 - p)\n",
        "\n",
        "    # gradient w.r.t y_pred\n",
        "    def gradient(self, y, y_pred):\n",
        "        p = self.log_func(y_pred)\n",
        "        return -(y - p)\n",
        "\n",
        "    # w.r.t y_pred\n",
        "    def hessian(self, y, y_pred):\n",
        "        p = self.log_func(y_pred)\n",
        "        return p * (1 - p)\n",
        "\n",
        "def to_categorical(x, n_col=None):\n",
        "    if not n_col:\n",
        "        n_col = np.amax(x) + 1\n",
        "    one_hot = np.zeros((x.shape[0], n_col))\n",
        "    one_hot[np.arange(x.shape[0]), x] = 1\n",
        "    return one_hot\n",
        "\n",
        "def normalize(X, axis=-1, order=2):\n",
        "    l2 = np.atleast_1d(np.linalg.norm(X, order, axis))\n",
        "    print(l2)\n",
        "    l2[l2 == 0] = 1\n",
        "    return X / np.expand_dims(l2, axis)\n",
        "\n",
        "class TanH():\n",
        "    def __call__(self, x):\n",
        "        return 2 / (1 + np.exp(-2*x)) - 1\n",
        "    def gradient(self, x):\n",
        "        return 1 - np.power(self.__call__(x), 2)\n",
        "\n",
        "class ReLU():\n",
        "    def __call__(self, x):\n",
        "        return np.where(x >= 0, x, 0)\n",
        "    def gradient(self, x):\n",
        "        return np.where(x >= 0, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HLtmOLGM9tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron():\n",
        "    def __init__(self, n_iterations=20000, activation_function=Sigmoid, loss=SquareLoss,initialize=\"xa\", learning_rate=0.01):\n",
        "        self.n_iterations = n_iterations\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss = loss()\n",
        "        self.activation_func = activation_function()\n",
        "        self.progressbar = progressbar.ProgressBar(widgets= [\n",
        "                    'Training: ', progressbar.Percentage(), ' ', progressbar.Bar(marker=\"-\", left=\"[\", right=\"]\"),\n",
        "                    ' ', progressbar.ETA()])\n",
        "        self.initialize=initialize\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = np.shape(X)\n",
        "        _, n_outputs = np.shape(y)\n",
        "\n",
        "        if self.initialize==\"xa\":\n",
        "          limit = 1 / math.sqrt(n_features)\n",
        "          self.W = np.random.uniform(-limit, limit, (n_features, n_outputs))\n",
        "        elif self.initialize=\"zero\":\n",
        "          self.W = np.zeroes((n_features, n_outputs))\n",
        "        elif self.initialize=\"he\":\n",
        "          limit = 2 / math.sqrt(n_features)\n",
        "          self.W = np.random.uniform(-limit, limit, (n_features, n_outputs))\n",
        "        \n",
        "        \n",
        "        self.w0 = np.zeros((1, n_outputs))\n",
        "\n",
        "        for i in self.progressbar(range(self.n_iterations)):\n",
        "            linear_output = X.dot(self.W) + self.w0\n",
        "            y_pred = self.activation_func(linear_output)\n",
        "            error_gradient = self.loss.gradient(y, y_pred) * self.activation_func.gradient(linear_output)\n",
        "            grad_wrt_w = X.T.dot(error_gradient)\n",
        "            grad_wrt_w0 = np.sum(error_gradient, axis=0, keepdims=True)\n",
        "            self.W  -= self.learning_rate * grad_wrt_w\n",
        "            self.w0 -= self.learning_rate  * grad_wrt_w0\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.activation_func(X.dot(self.W) + self.w0)\n",
        "        return y_pred\n",
        "\n",
        "class Layer(object):\n",
        "\n",
        "    def set_input_shape(self, shape):\n",
        "        self.input_shape = shape\n",
        "\n",
        "    def layer_name(self):\n",
        "        return self.__class__.__name__\n",
        "\n",
        "    def parameters(self):\n",
        "        return 0\n",
        "\n",
        "    def forward_pass(self, X, training):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def output_shape(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, n_units, input_shape=None):\n",
        "        self.layer_input = None\n",
        "        self.input_shape = input_shape\n",
        "        self.n_units = n_units\n",
        "        self.trainable = True\n",
        "        self.W = None\n",
        "        self.w0 = None\n",
        "\n",
        "    def initialize(self, optimizer):\n",
        "        # Initialize the weights\n",
        "        limit = 1 / math.sqrt(self.input_shape[0])\n",
        "        self.W  = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n",
        "        self.w0 = np.zeros((1, self.n_units))\n",
        "        # Weight optimizers\n",
        "        self.W_opt  = copy.copy(optimizer)\n",
        "        self.w0_opt = copy.copy(optimizer)\n",
        "\n",
        "    def parameters(self):\n",
        "        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n",
        "\n",
        "    def forward_pass(self, X, training=True):\n",
        "        self.layer_input = X\n",
        "        return X.dot(self.W) + self.w0\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        W = self.W\n",
        "        if self.trainable:\n",
        "            grad_w = self.layer_input.T.dot(accum_grad)\n",
        "            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n",
        "\n",
        "            # Update the layer weights\n",
        "            self.W = self.W_opt.update(self.W, grad_w)\n",
        "            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n",
        "\n",
        "        accum_grad = accum_grad.dot(W.T)\n",
        "        return accum_grad\n",
        "\n",
        "    def output_shape(self):\n",
        "        return (self.n_units, )\n",
        "\n",
        "\n",
        "class RNN(Layer):\n",
        "    #http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/\n",
        "    def __init__(self, n_units, activation='tanh', bptt_trunc=5, input_shape=None):\n",
        "        self.input_shape = input_shape\n",
        "        self.n_units = n_units\n",
        "        self.activation = activation_functions[activation]()\n",
        "        self.trainable = True\n",
        "        self.bptt_trunc = bptt_trunc\n",
        "        self.W = None # Weight of the previous state\n",
        "        self.V = None # Weight of the output\n",
        "        self.U = None # Weight of the input\n",
        "\n",
        "    def initialize(self, optimizer):\n",
        "        timesteps, input_dim = self.input_shape\n",
        "        # Initialize the weights\n",
        "        limit = 1 / math.sqrt(input_dim)\n",
        "        self.U  = np.random.uniform(-limit, limit, (self.n_units, input_dim))\n",
        "        limit = 1 / math.sqrt(self.n_units)\n",
        "        self.V = np.random.uniform(-limit, limit, (input_dim, self.n_units))\n",
        "        self.W  = np.random.uniform(-limit, limit, (self.n_units, self.n_units))\n",
        "        # Weight optimizers\n",
        "        self.U_opt  = copy.copy(optimizer)\n",
        "        self.V_opt = copy.copy(optimizer)\n",
        "        self.W_opt = copy.copy(optimizer)\n",
        "\n",
        "    def parameters(self):\n",
        "        return np.prod(self.W.shape) + np.prod(self.U.shape) + np.prod(self.V.shape)\n",
        "\n",
        "    def forward_pass(self, X, training=True):\n",
        "        self.layer_input = X\n",
        "        batch_size, timesteps, input_dim = X.shape\n",
        "\n",
        "        self.state_input = np.zeros((batch_size, timesteps, self.n_units))\n",
        "        self.states = np.zeros((batch_size, timesteps+1, self.n_units))\n",
        "        self.outputs = np.zeros((batch_size, timesteps, input_dim))\n",
        "\n",
        "        self.states[:, -1] = np.zeros((batch_size, self.n_units))\n",
        "        for t in range(timesteps):\n",
        "            self.state_input[:, t] = X[:, t].dot(self.U.T) + self.states[:, t-1].dot(self.W.T)\n",
        "            self.states[:, t] = self.activation(self.state_input[:, t])\n",
        "            self.outputs[:, t] = self.states[:, t].dot(self.V.T)\n",
        "\n",
        "        return self.outputs\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        _, timesteps, _ = accum_grad.shape\n",
        "\n",
        "        grad_U = np.zeros_like(self.U)\n",
        "        grad_V = np.zeros_like(self.V)\n",
        "        grad_W = np.zeros_like(self.W)\n",
        "        accum_grad_next = np.zeros_like(accum_grad)\n",
        "\n",
        "        for t in reversed(range(timesteps)):\n",
        "            grad_V += accum_grad[:, t].T.dot(self.states[:, t])\n",
        "            grad_wrt_state = accum_grad[:, t].dot(self.V) * self.activation.gradient(self.state_input[:, t])\n",
        "            accum_grad_next[:, t] = grad_wrt_state.dot(self.U)\n",
        "            for t_ in reversed(np.arange(max(0, t - self.bptt_trunc), t+1)):\n",
        "                grad_U += grad_wrt_state.T.dot(self.layer_input[:, t_])\n",
        "                grad_W += grad_wrt_state.T.dot(self.states[:, t_-1])\n",
        "                grad_wrt_state = grad_wrt_state.dot(self.W) * self.activation.gradient(self.state_input[:, t_-1])\n",
        "\n",
        "        # Update weights\n",
        "        self.U = self.U_opt.update(self.U, grad_U)\n",
        "        self.V = self.V_opt.update(self.V, grad_V)\n",
        "        self.W = self.W_opt.update(self.W, grad_W)\n",
        "        return accum_grad_next\n",
        "\n",
        "    def output_shape(self):\n",
        "        return self.input_shape\n",
        "\n",
        "class Conv2D(Layer):\n",
        "    def __init__(self, n_filters, filter_shape, input_shape=None, padding='same', stride=1):\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_shape = filter_shape\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "        self.input_shape = input_shape\n",
        "        self.trainable = True\n",
        "\n",
        "    def initialize(self, optimizer):\n",
        "        # Initialize the weights\n",
        "        filter_height, filter_width = self.filter_shape\n",
        "        channels = self.input_shape[0]\n",
        "        limit = 1 / math.sqrt(np.prod(self.filter_shape))\n",
        "        self.W  = np.random.uniform(-limit, limit, size=(self.n_filters, channels, filter_height, filter_width))\n",
        "        self.w0 = np.zeros((self.n_filters, 1))\n",
        "        # Weight optimizers\n",
        "        self.W_opt  = copy.copy(optimizer)\n",
        "        self.w0_opt = copy.copy(optimizer)\n",
        "\n",
        "    def parameters(self):\n",
        "        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n",
        "\n",
        "    def forward_pass(self, X, training=True):\n",
        "        batch_size, channels, height, width = X.shape\n",
        "        self.layer_input = X\n",
        "        # Turn image shape into column shape\n",
        "        # (enables dot product between input and weights)\n",
        "        self.X_col = image_to_column(X, self.filter_shape, stride=self.stride, output_shape=self.padding)\n",
        "        # Turn weights into column shape\n",
        "        self.W_col = self.W.reshape((self.n_filters, -1))\n",
        "        # Calculate output\n",
        "        output = self.W_col.dot(self.X_col) + self.w0\n",
        "        # Reshape into (n_filters, out_height, out_width, batch_size)\n",
        "        output = output.reshape(self.output_shape() + (batch_size, ))\n",
        "        # Redistribute axises so that batch size comes first\n",
        "        return output.transpose(3,0,1,2)\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        # Reshape accumulated gradient into column shape\n",
        "        accum_grad = accum_grad.transpose(1, 2, 3, 0).reshape(self.n_filters, -1)\n",
        "\n",
        "        if self.trainable:\n",
        "            # Take dot product between column shaped accum. gradient and column shape\n",
        "            # layer input to determine the gradient at the layer with respect to layer weights\n",
        "            grad_w = accum_grad.dot(self.X_col.T).reshape(self.W.shape)\n",
        "            # The gradient with respect to bias terms is the sum similarly to in Dense layer\n",
        "            grad_w0 = np.sum(accum_grad, axis=1, keepdims=True)\n",
        "\n",
        "            # Update the layers weights\n",
        "            self.W = self.W_opt.update(self.W, grad_w)\n",
        "            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n",
        "\n",
        "        # Recalculate the gradient which will be propogated back to prev. layer\n",
        "        accum_grad = self.W_col.T.dot(accum_grad)\n",
        "        # Reshape from column shape to image shape\n",
        "        accum_grad = column_to_image(accum_grad,\n",
        "                                self.layer_input.shape,\n",
        "                                self.filter_shape,\n",
        "                                stride=self.stride,\n",
        "                                output_shape=self.padding)\n",
        "\n",
        "        return accum_grad\n",
        "\n",
        "    def output_shape(self):\n",
        "        channels, height, width = self.input_shape\n",
        "        pad_h, pad_w = determine_padding(self.filter_shape, output_shape=self.padding)\n",
        "        output_height = (height + np.sum(pad_h) - self.filter_shape[0]) / self.stride + 1\n",
        "        output_width = (width + np.sum(pad_w) - self.filter_shape[1]) / self.stride + 1\n",
        "        return self.n_filters, int(output_height), int(output_width)\n",
        "\n",
        "\n",
        "class BatchNormalization(Layer):\n",
        "    \"\"\"Batch normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, momentum=0.99):\n",
        "        self.momentum = momentum\n",
        "        self.trainable = True\n",
        "        self.eps = 0.01\n",
        "        self.running_mean = None\n",
        "        self.running_var = None\n",
        "\n",
        "    def initialize(self, optimizer):\n",
        "        # Initialize the parameters\n",
        "        self.gamma  = np.ones(self.input_shape)\n",
        "        self.beta = np.zeros(self.input_shape)\n",
        "        # parameter optimizers\n",
        "        self.gamma_opt  = copy.copy(optimizer)\n",
        "        self.beta_opt = copy.copy(optimizer)\n",
        "\n",
        "    def parameters(self):\n",
        "        return np.prod(self.gamma.shape) + np.prod(self.beta.shape)\n",
        "\n",
        "    def forward_pass(self, X, training=True):\n",
        "\n",
        "        # Initialize running mean and variance if first run\n",
        "        if self.running_mean is None:\n",
        "            self.running_mean = np.mean(X, axis=0)\n",
        "            self.running_var = np.var(X, axis=0)\n",
        "\n",
        "        if training and self.trainable:\n",
        "            mean = np.mean(X, axis=0)\n",
        "            var = np.var(X, axis=0)\n",
        "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
        "        else:\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        # Statistics saved for backward pass\n",
        "        self.X_centered = X - mean\n",
        "        self.stddev_inv = 1 / np.sqrt(var + self.eps)\n",
        "\n",
        "        X_norm = self.X_centered * self.stddev_inv\n",
        "        output = self.gamma * X_norm + self.beta\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "\n",
        "        # Save parameters used during the forward pass\n",
        "        gamma = self.gamma\n",
        "\n",
        "        # If the layer is trainable the parameters are updated\n",
        "        if self.trainable:\n",
        "            X_norm = self.X_centered * self.stddev_inv\n",
        "            grad_gamma = np.sum(accum_grad * X_norm, axis=0)\n",
        "            grad_beta = np.sum(accum_grad, axis=0)\n",
        "\n",
        "            self.gamma = self.gamma_opt.update(self.gamma, grad_gamma)\n",
        "            self.beta = self.beta_opt.update(self.beta, grad_beta)\n",
        "\n",
        "        batch_size = accum_grad.shape[0]\n",
        "\n",
        "        # The gradient of the loss with respect to the layer inputs (use weights and statistics from forward pass)\n",
        "        accum_grad = (1 / batch_size) * gamma * self.stddev_inv * (\n",
        "            batch_size * accum_grad\n",
        "            - np.sum(accum_grad, axis=0)\n",
        "            - self.X_centered * self.stddev_inv**2 * np.sum(accum_grad * self.X_centered, axis=0)\n",
        "            )\n",
        "\n",
        "        return accum_grad\n",
        "\n",
        "    def output_shape(self):\n",
        "        return self.input_shape\n",
        "\n",
        "\n",
        "class PoolingLayer(Layer):\n",
        "    \"\"\"A parent class of MaxPooling2D and AveragePooling2D\n",
        "    \"\"\"\n",
        "    def __init__(self, pool_shape=(2, 2), stride=1, padding=0):\n",
        "        self.pool_shape = pool_shape\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.trainable = True\n",
        "\n",
        "    def forward_pass(self, X, training=True):\n",
        "        self.layer_input = X\n",
        "\n",
        "        batch_size, channels, height, width = X.shape\n",
        "\n",
        "        _, out_height, out_width = self.output_shape()\n",
        "\n",
        "        X = X.reshape(batch_size*channels, 1, height, width)\n",
        "        X_col = image_to_column(X, self.pool_shape, self.stride, self.padding)\n",
        "\n",
        "        # MaxPool or AveragePool specific method\n",
        "        output = self._pool_forward(X_col)\n",
        "\n",
        "        output = output.reshape(out_height, out_width, batch_size, channels)\n",
        "        output = output.transpose(2, 3, 0, 1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        batch_size, _, _, _ = accum_grad.shape\n",
        "        channels, height, width = self.input_shape\n",
        "        accum_grad = accum_grad.transpose(2, 3, 0, 1).ravel()\n",
        "\n",
        "        # MaxPool or AveragePool specific method\n",
        "        accum_grad_col = self._pool_backward(accum_grad)\n",
        "\n",
        "        accum_grad = column_to_image(accum_grad_col, (batch_size * channels, 1, height, width), self.pool_shape, self.stride, 0)\n",
        "        accum_grad = accum_grad.reshape((batch_size,) + self.input_shape)\n",
        "\n",
        "        return accum_grad\n",
        "\n",
        "    def output_shape(self):\n",
        "        channels, height, width = self.input_shape\n",
        "        out_height = (height - self.pool_shape[0]) / self.stride + 1\n",
        "        out_width = (width - self.pool_shape[1]) / self.stride + 1\n",
        "        assert out_height % 1 == 0\n",
        "        assert out_width % 1 == 0\n",
        "        return channels, int(out_height), int(out_width)\n",
        "\n",
        "\n",
        "class MaxPooling2D(PoolingLayer):\n",
        "    def _pool_forward(self, X_col):\n",
        "        arg_max = np.argmax(X_col, axis=0).flatten()\n",
        "        output = X_col[arg_max, range(arg_max.size)]\n",
        "        self.cache = arg_max\n",
        "        return output\n",
        "\n",
        "    def _pool_backward(self, accum_grad):\n",
        "        accum_grad_col = np.zeros((np.prod(self.pool_shape), accum_grad.size))\n",
        "        arg_max = self.cache\n",
        "        accum_grad_col[arg_max, range(accum_grad.size)] = accum_grad\n",
        "        return accum_grad_col\n",
        "\n",
        "class AveragePooling2D(PoolingLayer):\n",
        "    def _pool_forward(self, X_col):\n",
        "        output = np.mean(X_col, axis=0)\n",
        "        return output\n",
        "\n",
        "    def _pool_backward(self, accum_grad):\n",
        "        accum_grad_col = np.zeros((np.prod(self.pool_shape), accum_grad.size))\n",
        "        accum_grad_col[:, range(accum_grad.size)] = 1. / accum_grad_col.shape[0] * accum_grad\n",
        "        return accum_grad_col\n",
        "\n",
        "\n",
        "class ConstantPadding2D(Layer):\n",
        "    \"\"\"Adds rows and columns of constant values to the input.\n",
        "    Expects the input to be of shape (batch_size, channels, height, width)\n",
        "    Parameters:\n",
        "    -----------\n",
        "    padding: tuple\n",
        "        The amount of padding along the height and width dimension of the input.\n",
        "        If (pad_h, pad_w) the same symmetric padding is applied along height and width dimension.\n",
        "        If ((pad_h0, pad_h1), (pad_w0, pad_w1)) the specified padding is added to beginning and end of\n",
        "        the height and width dimension.\n",
        "    padding_value: int or tuple\n",
        "        The value the is added as padding.\n",
        "    \"\"\"\n",
        "    def __init__(self, padding, padding_value=0):\n",
        "        self.padding = padding\n",
        "        self.trainable = True\n",
        "        if not isinstance(padding[0], tuple):\n",
        "            self.padding = ((padding[0], padding[0]), padding[1])\n",
        "        if not isinstance(padding[1], tuple):\n",
        "            self.padding = (self.padding[0], (padding[1], padding[1]))\n",
        "        self.padding_value = padding_value\n",
        "\n",
        "    def forward_pass(self, X, training=True):\n",
        "        output = np.pad(X,\n",
        "            pad_width=((0,0), (0,0), self.padding[0], self.padding[1]),\n",
        "            mode=\"constant\",\n",
        "            constant_values=self.padding_value)\n",
        "        return output\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        pad_top, pad_left = self.padding[0][0], self.padding[1][0]\n",
        "        height, width = self.input_shape[1], self.input_shape[2]\n",
        "        accum_grad = accum_grad[:, :, pad_top:pad_top+height, pad_left:pad_left+width]\n",
        "        return accum_grad\n",
        "\n",
        "    def output_shape(self):\n",
        "        new_height = self.input_shape[1] + np.sum(self.padding[0])\n",
        "        new_width = self.input_shape[2] + np.sum(self.padding[1])\n",
        "        return (self.input_shape[0], new_height, new_width)\n",
        "\n",
        "\n",
        "class ZeroPadding2D(ConstantPadding2D):\n",
        "    \"\"\"Adds rows and columns of zero values to the input.\n",
        "    Expects the input to be of shape (batch_size, channels, height, width)\n",
        "    Parameters:\n",
        "    -----------\n",
        "    padding: tuple\n",
        "        The amount of padding along the height and width dimension of the input.\n",
        "        If (pad_h, pad_w) the same symmetric padding is applied along height and width dimension.\n",
        "        If ((pad_h0, pad_h1), (pad_w0, pad_w1)) the specified padding is added to beginning and end of\n",
        "        the height and width dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, padding):\n",
        "        self.padding = padding\n",
        "        if isinstance(padding[0], int):\n",
        "            self.padding = ((padding[0], padding[0]), padding[1])\n",
        "        if isinstance(padding[1], int):\n",
        "            self.padding = (self.padding[0], (padding[1], padding[1]))\n",
        "        self.padding_value = 0\n",
        "\n",
        "\n",
        "class Flatten(Layer):\n",
        "    def __init__(self, input_shape=None):\n",
        "        self.prev_shape = None\n",
        "        self.trainable = True\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def forward_pass(self, X, training=True):\n",
        "        self.prev_shape = X.shape\n",
        "        return X.reshape((X.shape[0], -1))\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        return accum_grad.reshape(self.prev_shape)\n",
        "\n",
        "    def output_shape(self):\n",
        "        return (np.prod(self.input_shape),)\n",
        "\n",
        "\n",
        "class UpSampling2D(Layer):\n",
        "    def __init__(self, size=(2,2), input_shape=None):\n",
        "        self.prev_shape = None\n",
        "        self.trainable = True\n",
        "        self.size = size\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def forward_pass(self, X, training=True):\n",
        "        self.prev_shape = X.shape\n",
        "        # Repeat each axis as specified by size\n",
        "        X_new = X.repeat(self.size[0], axis=2).repeat(self.size[1], axis=3)\n",
        "        return X_new\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        # Down sample input to previous shape\n",
        "        accum_grad = accum_grad[:, :, ::self.size[0], ::self.size[1]]\n",
        "        return accum_grad\n",
        "\n",
        "    def output_shape(self):\n",
        "        channels, height, width = self.input_shape\n",
        "        return channels, self.size[0] * height, self.size[1] * width\n",
        "\n",
        "\n",
        "class Reshape(Layer):\n",
        "    def __init__(self, shape, input_shape=None):\n",
        "        self.prev_shape = None\n",
        "        self.trainable = True\n",
        "        self.shape = shape\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def forward_pass(self, X, training=True):\n",
        "        self.prev_shape = X.shape\n",
        "        return X.reshape((X.shape[0], ) + self.shape)\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        return accum_grad.reshape(self.prev_shape)\n",
        "\n",
        "    def output_shape(self):\n",
        "        return self.shape\n",
        "\n",
        "\n",
        "class Dropout(Layer):\n",
        "    def __init__(self, p=0.2):\n",
        "        self.p = p\n",
        "        self._mask = None\n",
        "        self.input_shape = None\n",
        "        self.n_units = None\n",
        "        self.pass_through = True\n",
        "        self.trainable = True\n",
        "\n",
        "    def forward_pass(self, X, training=True):\n",
        "        c = (1 - self.p)\n",
        "        if training:\n",
        "            self._mask = np.random.uniform(size=X.shape) > self.p\n",
        "            c = self._mask\n",
        "        return X * c\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        return accum_grad * self._mask\n",
        "\n",
        "    def output_shape(self):\n",
        "        return self.input_shape\n",
        "\n",
        "activation_functions = {\n",
        "    'relu': ReLU,\n",
        "    'sigmoid': Sigmoid,\n",
        "    'selu': SELU,\n",
        "    'elu': ELU,\n",
        "    'softmax': Softmax,\n",
        "    'leaky_relu': LeakyReLU,\n",
        "    'tanh': TanH,\n",
        "    'softplus': SoftPlus\n",
        "}\n",
        "\n",
        "class Activation(Layer):\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.activation_name = name\n",
        "        self.activation_func = activation_functions[name]()\n",
        "        self.trainable = True\n",
        "\n",
        "    def layer_name(self):\n",
        "        return \"Activation (%s)\" % (self.activation_func.__class__.__name__)\n",
        "\n",
        "    def forward_pass(self, X, training=True):\n",
        "        self.layer_input = X\n",
        "        return self.activation_func(X)\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        return accum_grad * self.activation_func.gradient(self.layer_input)\n",
        "\n",
        "    def output_shape(self):\n",
        "        return self.input_shape\n",
        "\n",
        "\n",
        "def determine_padding(filter_shape, output_shape=\"same\"):\n",
        "    if output_shape == \"valid\":\n",
        "        return (0, 0), (0, 0)\n",
        "    elif output_shape == \"same\":\n",
        "        filter_height, filter_width = filter_shape\n",
        "\n",
        "        # Derived from:\n",
        "        # output_height = (height + pad_h - filter_height) / stride + 1\n",
        "        # In this case output_height = height and stride = 1. This gives the\n",
        "        # expression for the padding below.\n",
        "        pad_h1 = int(math.floor((filter_height - 1)/2))\n",
        "        pad_h2 = int(math.ceil((filter_height - 1)/2))\n",
        "        pad_w1 = int(math.floor((filter_width - 1)/2))\n",
        "        pad_w2 = int(math.ceil((filter_width - 1)/2))\n",
        "\n",
        "        return (pad_h1, pad_h2), (pad_w1, pad_w2)\n",
        "\n",
        "\n",
        "# Reference: CS231n Stanford\n",
        "def get_im2col_indices(images_shape, filter_shape, padding, stride=1):\n",
        "    # First figure out what the size of the output should be\n",
        "    batch_size, channels, height, width = images_shape\n",
        "    filter_height, filter_width = filter_shape\n",
        "    pad_h, pad_w = padding\n",
        "    out_height = int((height + np.sum(pad_h) - filter_height) / stride + 1)\n",
        "    out_width = int((width + np.sum(pad_w) - filter_width) / stride + 1)\n",
        "\n",
        "    i0 = np.repeat(np.arange(filter_height), filter_width)\n",
        "    i0 = np.tile(i0, channels)\n",
        "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
        "    j0 = np.tile(np.arange(filter_width), filter_height * channels)\n",
        "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
        "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
        "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
        "\n",
        "    k = np.repeat(np.arange(channels), filter_height * filter_width).reshape(-1, 1)\n",
        "\n",
        "    return (k, i, j)\n",
        "\n",
        "\n",
        "# Method which turns the image shaped input to column shape.\n",
        "# Used during the forward pass.\n",
        "# Reference: CS231n Stanford\n",
        "def image_to_column(images, filter_shape, stride, output_shape='same'):\n",
        "    filter_height, filter_width = filter_shape\n",
        "\n",
        "    pad_h, pad_w = determine_padding(filter_shape, output_shape)\n",
        "\n",
        "    # Add padding to the image\n",
        "    images_padded = np.pad(images, ((0, 0), (0, 0), pad_h, pad_w), mode='constant')\n",
        "\n",
        "    # Calculate the indices where the dot products are to be applied between weights\n",
        "    # and the image\n",
        "    k, i, j = get_im2col_indices(images.shape, filter_shape, (pad_h, pad_w), stride)\n",
        "\n",
        "    # Get content from image at those indices\n",
        "    cols = images_padded[:, k, i, j]\n",
        "    channels = images.shape[1]\n",
        "    # Reshape content into column shape\n",
        "    cols = cols.transpose(1, 2, 0).reshape(filter_height * filter_width * channels, -1)\n",
        "    return cols\n",
        "\n",
        "\n",
        "\n",
        "# Method which turns the column shaped input to image shape.\n",
        "# Used during the backward pass.\n",
        "# Reference: CS231n Stanford\n",
        "def column_to_image(cols, images_shape, filter_shape, stride, output_shape='same'):\n",
        "    batch_size, channels, height, width = images_shape\n",
        "    pad_h, pad_w = determine_padding(filter_shape, output_shape)\n",
        "    height_padded = height + np.sum(pad_h)\n",
        "    width_padded = width + np.sum(pad_w)\n",
        "    images_padded = np.zeros((batch_size, channels, height_padded, width_padded))\n",
        "\n",
        "    # Calculate the indices where the dot products are applied between weights\n",
        "    # and the image\n",
        "    k, i, j = get_im2col_indices(images_shape, filter_shape, (pad_h, pad_w), stride)\n",
        "\n",
        "    cols = cols.reshape(channels * np.prod(filter_shape), -1, batch_size)\n",
        "    cols = cols.transpose(2, 0, 1)\n",
        "    # Add column content to the images at the indices\n",
        "    np.add.at(images_padded, (slice(None), k, i, j), cols)\n",
        "\n",
        "    # Return image without padding\n",
        "    return images_padded[:, :, pad_h[0]:height+pad_h[0], pad_w[0]:width+pad_w[0]]\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self, optimizer, loss, validation_data=None):\n",
        "        self.optimizer = optimizer\n",
        "        self.layers = []\n",
        "        self.errors = {\"training\": [], \"validation\": []}\n",
        "        self.loss_function = loss()\n",
        "        self.progressbar = progressbar.ProgressBar(widgets= [\n",
        "                    'Training: ', progressbar.Percentage(), ' ', progressbar.Bar(marker=\"-\", left=\"[\", right=\"]\"),\n",
        "                    ' ', progressbar.ETA()])\n",
        "        self.val_set = None\n",
        "        if validation_data:\n",
        "            X, y = validation_data\n",
        "            self.val_set = {\"X\": X, \"y\": y}\n",
        "\n",
        "    def set_trainable(self, trainable):\n",
        "        for layer in self.layers:\n",
        "            layer.trainable = trainable\n",
        "\n",
        "    def add(self, layer):\n",
        "        if self.layers:\n",
        "            layer.set_input_shape(shape=self.layers[-1].output_shape())\n",
        "\n",
        "        if hasattr(layer, 'initialize'):\n",
        "            layer.initialize(optimizer=self.optimizer)\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def test_on_batch(self, X, y):\n",
        "        y_pred = self._forward_pass(X, training=False)\n",
        "        loss = np.mean(self.loss_function.loss(y, y_pred))\n",
        "        acc = self.loss_function.acc(y, y_pred)\n",
        "\n",
        "        return loss, acc\n",
        "\n",
        "    def train_on_batch(self, X, y):\n",
        "        y_pred = self._forward_pass(X)\n",
        "        loss = np.mean(self.loss_function.loss(y, y_pred))\n",
        "        acc = self.loss_function.acc(y, y_pred)\n",
        "        loss_grad = self.loss_function.gradient(y, y_pred)\n",
        "        self._backward_pass(loss_grad=loss_grad)\n",
        "        return loss, acc\n",
        "\n",
        "    def fit(self, X, y, n_epochs, batch_size):\n",
        "        for _ in self.progressbar(range(n_epochs)):\n",
        "            batch_error = []\n",
        "            for X_batch, y_batch in batch_iterator(X, y, batch_size=batch_size):\n",
        "                loss, _ = self.train_on_batch(X_batch, y_batch)\n",
        "                batch_error.append(loss)\n",
        "            self.errors[\"training\"].append(np.mean(batch_error))\n",
        "            if self.val_set is not None:\n",
        "                val_loss, _ = self.test_on_batch(self.val_set[\"X\"], self.val_set[\"y\"])\n",
        "                self.errors[\"validation\"].append(val_loss)\n",
        "\n",
        "        return self.errors[\"training\"], self.errors[\"validation\"]\n",
        "\n",
        "    def _forward_pass(self, X, training=True):\n",
        "        layer_output = X\n",
        "        for layer in self.layers:\n",
        "            layer_output = layer.forward_pass(layer_output, training)\n",
        "        return layer_output\n",
        "\n",
        "    def _backward_pass(self, loss_grad):\n",
        "        for layer in reversed(self.layers):\n",
        "            loss_grad = layer.backward_pass(loss_grad)\n",
        "\n",
        "    def summary(self, name=\"Model Summary\"):\n",
        "        print (AsciiTable([[name]]).table)\n",
        "        print (\"Input Shape: %s\" % str(self.layers[0].input_shape))\n",
        "        table_data = [[\"Layer Type\", \"Parameters\", \"Output Shape\"]]\n",
        "        tot_params = 0\n",
        "        for layer in self.layers:\n",
        "            layer_name = layer.layer_name()\n",
        "            params = layer.parameters()\n",
        "            out_shape = layer.output_shape()\n",
        "            table_data.append([layer_name, str(params), str(out_shape)])\n",
        "            tot_params += params\n",
        "        print (AsciiTable(table_data).table)\n",
        "        print (\"Total Parameters: %d\\n\" % tot_params)\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        return self._forward_pass(X, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1eKpFdOS3eN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "584887d1-5ebf-4667-c53d-58f3f709088c"
      },
      "source": [
        "data = datasets.load_digits()\n",
        "X = normalize(data.data)\n",
        "y = data.target\n",
        "\n",
        "y = to_categorical(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "\n",
        "# Perceptron\n",
        "clf = Perceptron(n_iterations=5000,\n",
        "    learning_rate=0.001, \n",
        "    loss=CrossEntropy,\n",
        "    activation_function=Sigmoid)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = np.argmax(clf.predict(X_test), axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print (\"Accuracy:\", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training:   0% [                                               ] ETA:   0:00:10"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[55.40758071 64.87680633 66.24198065 ... 71.3582511  65.69627082\n",
            " 70.27090436]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Training: 100% [-----------------------------------------------] Time:  0:00:09\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9562289562289562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imk9bX8HWTo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}