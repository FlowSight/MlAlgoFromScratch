{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tree.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpTTBhCfDuX5hTI15QXjEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlowSight/MlAlgoFromScratch/blob/master/Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoTydEnqqDnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.metrics import log_loss,roc_auc_score\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,mean_squared_error\n",
        "from collections import OrderedDict\n",
        "from scipy import linalg\n",
        "import numpy as np\n",
        "from itertools import combinations_with_replacement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDgVJ6cGpWHe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "7d2624fe-107c-4f19-9ba0-d67891c1714d"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/FlowSight/dataset/master/data_banknote_authentication.txt\n",
        "!wget https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/data/TempLinkoping2016.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-26 21:04:15--  https://raw.githubusercontent.com/FlowSight/dataset/master/data_banknote_authentication.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46400 (45K) [text/plain]\n",
            "Saving to: ‘data_banknote_authentication.txt’\n",
            "\n",
            "\r          data_bank   0%[                    ]       0  --.-KB/s               \rdata_banknote_authe 100%[===================>]  45.31K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-04-26 21:04:16 (2.17 MB/s) - ‘data_banknote_authentication.txt’ saved [46400/46400]\n",
            "\n",
            "--2020-04-26 21:04:17--  https://raw.githubusercontent.com/eriklindernoren/ML-From-Scratch/master/mlfromscratch/data/TempLinkoping2016.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6024 (5.9K) [text/plain]\n",
            "Saving to: ‘TempLinkoping2016.txt’\n",
            "\n",
            "TempLinkoping2016.t 100%[===================>]   5.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-04-26 21:04:17 (86.0 MB/s) - ‘TempLinkoping2016.txt’ saved [6024/6024]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpx8ZfQUn9Q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid():\n",
        "    def __call__(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def gradient(self, x):\n",
        "        return self.__call__(x) * (1 - self.__call__(x))\n",
        "\n",
        "class Softmax():\n",
        "    def __call__(self, x):\n",
        "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "    def gradient(self, x):\n",
        "        p = self.__call__(x)\n",
        "        return p * (1 - p)\n",
        "\n",
        "class Loss(object):\n",
        "    def loss(self, y_true, y_pred):\n",
        "        return NotImplementedError()\n",
        "\n",
        "    def gradient(self, y, y_pred):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def acc(self, y, y_pred):\n",
        "        return 0\n",
        "\n",
        "class SquareLoss(Loss):\n",
        "    def __init__(self): pass\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        return 0.5 * np.power((y - y_pred), 2)\n",
        "\n",
        "    def gradient(self, y, y_pred):\n",
        "        return -(y - y_pred)\n",
        "\n",
        "    def hessian(self, y, y_pred):\n",
        "        return 1\n",
        "\n",
        "\n",
        "class CrossEntropy(Loss):\n",
        "    def __init__(self): pass\n",
        "\n",
        "    def loss(self, y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
        "\n",
        "    def acc(self, y, p):\n",
        "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
        "\n",
        "    def gradient(self, y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - (y / p) + (1 - y) / (1 - p)\n",
        "\n",
        "    def hessian(self,y,p):\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return y/(p**2) + (1-y)/(1-p)**2\n",
        "\n",
        "class LogisticLoss():\n",
        "    def __init__(self):\n",
        "        sigmoid = Sigmoid()\n",
        "        self.log_func = sigmoid\n",
        "        self.log_grad = sigmoid.gradient\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        p = self.log_func(y_pred)\n",
        "        return y * np.log(p) + (1 - y) * np.log(1 - p)\n",
        "\n",
        "    # gradient w.r.t y_pred\n",
        "    def gradient(self, y, y_pred):\n",
        "        p = self.log_func(y_pred)\n",
        "        return -(y - p)\n",
        "\n",
        "    # w.r.t y_pred\n",
        "    def hessian(self, y, y_pred):\n",
        "        p = self.log_func(y_pred)\n",
        "        return p * (1 - p)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRvdUpKuoPf4",
        "colab_type": "text"
      },
      "source": [
        "Gradient Boosting forest implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdOD-Q0GoOA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import progressbar\n",
        "\n",
        "def split_on_feature(X, feature_i, feature_threhold):\n",
        "    split_func = None\n",
        "    if isinstance(feature_threhold, int) or isinstance(feature_threhold, float):\n",
        "        split_func = lambda sample: sample[feature_i] <= feature_threhold\n",
        "    else:\n",
        "        split_func = lambda sample: sample[feature_i] == feature_threhold\n",
        "\n",
        "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
        "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
        "\n",
        "    return np.array([X_1, X_2])\n",
        "\n",
        "def calculate_entropy(y):\n",
        "    log2 = lambda x: math.log(x) / math.log(2)\n",
        "    unique_labels = np.unique(y)\n",
        "    entropy = 0\n",
        "    for label in unique_labels:\n",
        "        count = len(y[y == label])\n",
        "        p = count / len(y)\n",
        "        entropy += -p * log2(p)\n",
        "    return entropy\n",
        "\n",
        "def calculate_variance(X):\n",
        "    mean = np.ones(np.shape(X)) * X.mean(0)\n",
        "    n_samples = np.shape(X)[0]\n",
        "    variance = (1 / n_samples) * np.diag((X - mean).T.dot(X - mean))\n",
        "    return variance\n",
        "\n",
        "def to_categorical(x, n_col=None):\n",
        "    if not n_col:\n",
        "        n_col = np.amax(x) + 1\n",
        "    one_hot = np.zeros((x.shape[0], n_col))\n",
        "    one_hot[np.arange(x.shape[0]), x] = 1\n",
        "    return one_hot\n",
        "\n",
        "class DecisionNode():\n",
        "    def __init__(self, feature_i=None, threshold=None,\n",
        "                 value=None, left=None, right=None, weight = None):\n",
        "        self.feature_i = feature_i          # Index for the feature that is tested\n",
        "        self.threshold = threshold          # Threshold value for feature\n",
        "        self.value = value                  # Value if the node is a leaf in the tree\n",
        "        self.left = left   \n",
        "        self.right = right\n",
        "        #self.weight - weight  # to be used for gbm/xgboost\n",
        "\n",
        "\n",
        "class DecisionTree(object):\n",
        "\n",
        "   def __init__(self,classify=False, min_samples_split=3, min_impurity=1e-5,\n",
        "                 max_depth=float(\"inf\"), loss=None, tree_lambda=1e-2):\n",
        "        self.root = None \n",
        "        self.classify=classify\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_impurity = min_impurity\n",
        "        self.max_depth = max_depth\n",
        "        if self.classify:\n",
        "          self.impurity_calculation = self.information_gain\n",
        "          self.leaf_value_calculation = self.majority_vote\n",
        "        else:\n",
        "          self.impurity_calculation = self.variance_reduction\n",
        "          self.leaf_value_calculation = self.mean_of_y\n",
        "        self.one_dim = None\n",
        "        self.loss = loss\n",
        "        self.tree_lambda = tree_lambda\n",
        "\n",
        "\n",
        "   def information_gain(self, y, y1, y2):\n",
        "        p = len(y1) / len(y)\n",
        "        entropy = calculate_entropy(y)\n",
        "        info_gain = entropy -p*calculate_entropy(y1) -(1-p)*calculate_entropy(y2)\n",
        "        return info_gain\n",
        "      \n",
        "   def variance_reduction(self, y, y1, y2):\n",
        "        var_tot = calculate_variance(y)\n",
        "        var_1 = calculate_variance(y1)\n",
        "        var_2 = calculate_variance(y2)\n",
        "        frac_1 = len(y1) / len(y)\n",
        "        frac_2 = len(y2) / len(y)\n",
        "        variance_reduction = var_tot - (frac_1 * var_1 + frac_2 * var_2)\n",
        "        return sum(variance_reduction)\n",
        "\n",
        "   def majority_vote(self, y):\n",
        "        most_common = None\n",
        "        max_count = 0\n",
        "        for label in np.unique(y):\n",
        "            count = len(y[y == label])\n",
        "            if count > max_count:\n",
        "                most_common = label\n",
        "                max_count = count\n",
        "        return most_common\n",
        "\n",
        "   def mean_of_y(self, y):\n",
        "        value = np.mean(y, axis=0)\n",
        "        return value if len(value) > 1 else value[0]\n",
        "\n",
        "   def fit(self, X, y, loss=None):\n",
        "        self.one_dim = len(np.shape(y)) == 1\n",
        "        self.root = self.build_tree(X, y)\n",
        "        self.loss=None\n",
        "\n",
        "   def build_tree(self, X, y, current_depth=0):\n",
        "        largest_impurity = 0\n",
        "        best_criteria = None    # Feature index and threshold\n",
        "        best_sets = None        # Subsets of the data\n",
        "\n",
        "        # sanitize y\n",
        "        if len(np.shape(y)) == 1:\n",
        "            y = np.expand_dims(y, axis=1)\n",
        "        np.reshape(y,(y.shape[0],y.shape[1]))\n",
        "      \n",
        "        Xy = np.concatenate((X, y), axis=1)\n",
        "        n_samples, n_features = np.shape(X)\n",
        "\n",
        "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
        "            for feature_i in range(n_features):\n",
        "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
        "                unique_values = np.unique(feature_values)\n",
        "                for threshold in unique_values:\n",
        "                    Xy1, Xy2 = split_on_feature(Xy, feature_i, threshold)\n",
        "\n",
        "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
        "                        # Select the y-values of the two sets\n",
        "                        y1 = Xy1[:, n_features:]\n",
        "                        y2 = Xy2[:, n_features:]\n",
        "\n",
        "                        # Calculate impurity\n",
        "                        impurity = self.impurity_calculation(y, y1, y2)\n",
        "                        if impurity > largest_impurity:\n",
        "                            largest_impurity = impurity\n",
        "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\n",
        "                            best_sets = {\n",
        "                                \"leftX\": Xy1[:, :n_features],   # X of left subtree\n",
        "                                \"lefty\": Xy1[:, n_features:],   # y of left subtree\n",
        "                                \"rightX\": Xy2[:, :n_features],  # X of right subtree\n",
        "                                \"righty\": Xy2[:, n_features:]   # y of right subtree\n",
        "                                }\n",
        "\n",
        "        if largest_impurity > self.min_impurity:\n",
        "            left_branch = self.build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth + 1)\n",
        "            right_branch = self.build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth + 1)\n",
        "            return DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
        "                                \"threshold\"], left=left_branch, right=right_branch)\n",
        "\n",
        "        leaf_value = self.leaf_value_calculation(y)\n",
        "\n",
        "        return DecisionNode(value=leaf_value)\n",
        "\n",
        "\n",
        "   def predict_value(self, x, tree=None):\n",
        "        if tree is None:\n",
        "            tree = self.root\n",
        "        # if leaf return value\n",
        "        if tree.value is not None:\n",
        "            return tree.value\n",
        "        feature_value = x[tree.feature_i]\n",
        "\n",
        "        branch = tree.right\n",
        "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
        "            if feature_value <= tree.threshold:\n",
        "                branch = tree.left\n",
        "        elif feature_value == tree.threshold:\n",
        "            branch = tree.left\n",
        "        return self.predict_value(x, branch)\n",
        "\n",
        "   def predict(self, X):\n",
        "        y_pred = [self.predict_value(sample) for sample in X]\n",
        "        return y_pred\n",
        "\n",
        "   def print_tree(self, tree=None, indent=\" \"):\n",
        "        if not tree:\n",
        "            tree = self.root\n",
        "\n",
        "        # If we're at leaf => print the label\n",
        "        if tree.value is not None:\n",
        "            print (tree.value)\n",
        "        else:\n",
        "            # Print test\n",
        "            print (\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
        "            print (\"%sT->\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.left, indent + indent)\n",
        "            print (\"%sF->\" % (indent), end=\"\")\n",
        "            self.print_tree(tree.right, indent + indent)\n",
        "\n",
        "class RegressionTree(DecisionTree):\n",
        "    def _variance_reduction(self, y, y1, y2):\n",
        "        var_tot = calculate_variance(y)\n",
        "        var_1 = calculate_variance(y1)\n",
        "        var_2 = calculate_variance(y2)\n",
        "        frac_1 = len(y1) / len(y)\n",
        "        frac_2 = len(y2) / len(y)\n",
        "\n",
        "        # Calculate the variance reduction\n",
        "        variance_reduction = var_tot - (frac_1 * var_1 + frac_2 * var_2)\n",
        "        return sum(variance_reduction)\n",
        "\n",
        "    def _mean_of_y(self, y):\n",
        "        value = np.mean(y, axis=0)\n",
        "        return value if len(value) > 1 else value[0]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.impurity_calculation = self._variance_reduction\n",
        "        self.leaf_value_calculation = self._mean_of_y\n",
        "        super(RegressionTree, self).fit(X, y)\n",
        "\n",
        "class GradientBoostingForest(object): \n",
        "    def __init__(self, n_estimators=200, learning_rate=1, min_samples_split=2,\n",
        "                 min_info_gain=1e-7,max_depth=2, debug=False,regression=True):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_impurity = min_info_gain\n",
        "        self.max_depth = max_depth\n",
        "        self.regression = regression\n",
        "        self.bar = progressbar.ProgressBar(widgets= [\n",
        "                    'Training: ', progressbar.Percentage(), ' ', progressbar.Bar(marker=\"-\", left=\"[\", right=\"]\"),\n",
        "                    ' ', progressbar.ETA()])\n",
        "        self.loss = SquareLoss()\n",
        "        if not self.regression:\n",
        "            self.loss = CrossEntropy()\n",
        "\n",
        "        # Initialize regression trees\n",
        "        self.trees = []\n",
        "        for _ in range(n_estimators):\n",
        "            tree = RegressionTree(\n",
        "                    min_samples_split=self.min_samples_split,\n",
        "                    min_impurity=self.min_impurity,\n",
        "                    max_depth=self.max_depth)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if self.regression==False:\n",
        "          y = to_categorical(y)\n",
        "          y_pred = np.full(np.shape(y), np.zeros((y.shape[0],1)))\n",
        "        else:\n",
        "          y = y.reshape(y.shape[0],1)\n",
        "          y_pred = np.zeros((y.shape[0],1))\n",
        "        for i in self.bar(range(self.n_estimators)):\n",
        "          # get dL/dy_i_pred for EACH sample \n",
        "            gradient = self.loss.gradient(y, y_pred)\n",
        "            self.trees[i].fit(X, gradient)\n",
        "            update = self.trees[i].predict(X)\n",
        "            if self.regression:\n",
        "              update = np.array(update).reshape(len(update),1)\n",
        "            #print(update.shape)\n",
        "            # Update y prediction\n",
        "            if i>0:\n",
        "              y_pred -= np.multiply(self.learning_rate, update)\n",
        "            else:\n",
        "              y_pred -= update\n",
        "  \n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.array([])\n",
        "        # Make predictions\n",
        "        idx=0\n",
        "        for tree in self.trees:\n",
        "            update = tree.predict(X)\n",
        "            if idx >0 :\n",
        "              update = np.multiply(self.learning_rate, update)\n",
        "            else:\n",
        "              update=np.array(update)\n",
        "            y_pred = -update if not y_pred.any() else y_pred - update\n",
        "            idx = idx+1\n",
        "\n",
        "        if not self.regression:\n",
        "            y_pred = np.exp(y_pred) / np.expand_dims(np.sum(np.exp(y_pred), axis=1), axis=1)\n",
        "            y_pred = np.argmax(y_pred, axis=1)\n",
        "        return y_pred\n",
        "\n",
        "class XGBoostTree(DecisionTree):\n",
        "\n",
        "    def split(self, y):\n",
        "        col = int(np.shape(y)[1]/2)\n",
        "        y, y_pred = y[:, :col], y[:, col:]\n",
        "        return y, y_pred\n",
        "\n",
        "    def gain(self, y, y_pred):\n",
        "        nominator = np.power((y * self.loss.gradient(y, y_pred)).sum(), 2)\n",
        "        denominator = self.loss.hessian(y, y_pred).sum()\n",
        "        return 0.5 * (nominator / (denominator+self.tree_lambda))\n",
        "\n",
        "    def gain_by_taylor(self, y, y1, y2):\n",
        "        # Split\n",
        "        y, y_pred = self.split(y)\n",
        "        y1, y1_pred = self.split(y1)\n",
        "        y2, y2_pred = self.split(y2)\n",
        "\n",
        "        true_gain = self.gain(y1, y1_pred)\n",
        "        false_gain = self.gain(y2, y2_pred)\n",
        "        gain = self.gain(y, y_pred)\n",
        "        return 0.5*(true_gain + false_gain - gain)-self.tree_lambda\n",
        "\n",
        "    def approximate_update(self, y):\n",
        "        y, y_pred = self.split(y)\n",
        "        gradient = np.sum(y * self.loss.gradient(y, y_pred), axis=0)\n",
        "        hessian = np.sum(self.loss.hessian(y, y_pred), axis=0)\n",
        "        update_approximation =  gradient / (hessian +self.tree_lambda)\n",
        "        return update_approximation\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.impurity_calculation = self.gain_by_taylor\n",
        "        self.leaf_value_calculation = self.approximate_update\n",
        "        super(XGBoostTree, self).fit(X, y)\n",
        "\n",
        "class XGBoostForest(object):\n",
        "    def __init__(self, n_estimators=200, learning_rate=0.001, min_samples_split=2,\n",
        "                 min_impurity=1e-7, max_depth=2,tree_lambda=0.05,classify=True):\n",
        "        self.n_estimators = n_estimators            # Number of trees\n",
        "        self.learning_rate = learning_rate          # Step size for weight update\n",
        "        self.min_samples_split = min_samples_split  # The minimum n of sampels to justify split\n",
        "        self.min_impurity = min_impurity              # Minimum variance reduction to continue\n",
        "        self.max_depth = max_depth                  # Maximum depth for tree\n",
        "\n",
        "        self.bar = progressbar.ProgressBar(widgets= [\n",
        "                    'Training: ', progressbar.Percentage(), ' ', progressbar.Bar(marker=\"-\", left=\"[\", right=\"]\"),\n",
        "                    ' ', progressbar.ETA()])\n",
        "        self.loss = LogisticLoss()\n",
        "        self.tree_lambda = tree_lambda\n",
        "        self.classify = classify\n",
        "        self.trees = []\n",
        "        for i in range(n_estimators):\n",
        "            tree = XGBoostTree(\n",
        "                    classify =self.classify,\n",
        "                    min_samples_split=self.min_samples_split,\n",
        "                    min_impurity=min_impurity,\n",
        "                    max_depth=self.max_depth,\n",
        "                    loss=self.loss,\n",
        "                    tree_lambda=self.tree_lambda)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if self.classify:\n",
        "          y = to_categorical(y)\n",
        "        else:\n",
        "          y = y.reshape(y.shape[0],1)\n",
        "        y_pred = np.zeros(np.shape(y)) #np.full(np.shape(y), np.zeros((y.shape[0],1)))\n",
        "        if self.classify:\n",
        "            self.num_classes = y.shape[1]\n",
        "\n",
        "        #y_pred = np.zeros(np.shape(y))\n",
        "        for i in self.bar(range(self.n_estimators)):\n",
        "            tree = self.trees[i]\n",
        "            y_and_pred = np.concatenate((y, y_pred), axis=1)\n",
        "            tree.fit(X, y_and_pred)\n",
        "            update_pred = tree.predict(X)\n",
        "            if i>0:\n",
        "              y_pred -= np.multiply(self.learning_rate, update_pred)\n",
        "            else:\n",
        "               y_pred -= update_pred\n",
        "            #print(\"y_pred: \"+str(y_pred))\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = None\n",
        "        idx=0\n",
        "        for tree in self.trees:\n",
        "            update = tree.predict(X)\n",
        "            #update = np.array(update).reshape(np.shape(X)[0],self.num_classes)\n",
        "            if y_pred is None:\n",
        "                y_pred = np.zeros_like(update)\n",
        "            if idx >0 :\n",
        "              update = np.multiply(self.learning_rate, update)\n",
        "            else:\n",
        "              update=np.array(update)\n",
        "            #print(\"update: \"+str(update))\n",
        "            y_pred = -update if not y_pred.any() else y_pred - update\n",
        "            idx = idx+1\n",
        "        #print(y_pred)\n",
        "        if self.classify:\n",
        "            y_pred = np.exp(y_pred) / np.sum(np.exp(y_pred), axis=1, keepdims=True)\n",
        "            y_pred = np.argmax(y_pred, axis=1)\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIZoSDspnL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d8a67317-ed0b-4ba0-c34e-4e11d6b69f26"
      },
      "source": [
        "data = datasets.load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
        "\n",
        "clf = GradientBoostingForest(n_estimators=50,learning_rate=0.05,max_depth=2,regression=False)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(y_pred)\n",
        "print(y_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print (\"Accuracy:\", accuracy)\n",
        "\n",
        "###################################################\n",
        "print(\"================Regression=================\")\n",
        "data = pd.read_csv('TempLinkoping2016.txt', sep=\"\\t\")\n",
        "\n",
        "time = np.atleast_2d(data[\"time\"].values).T\n",
        "temp = np.atleast_2d(data[\"temp\"].values).T\n",
        "\n",
        "X = time.reshape((-1, 1))               # Time. Fraction of the year [0, 1]\n",
        "X = np.insert(X, 0, values=1, axis=1)   # Insert bias term\n",
        "y = temp[:, 0]                          # Temperature. Reduce to one-dim\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
        "\n",
        "model = GradientBoostingForest(n_estimators=50,learning_rate=0.05,max_depth=2,regression=True)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_pred_line = model.predict(X)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print (\"Mean Squared Error:\", mse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: 100% [-----------------------------------------------] Time:  0:00:03\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2 0 1 2 2 1 1 1 0 0 1 1 0 1 2 1 2 0 0 0 1 2 2 2 2 0 0 2 1 2 2 2 1 0 0 1 2\n",
            " 1 0 1 1 1 0 1 2 1 1 2 2 2 1 2 1 2 1 2 1 0 1 0]\n",
            "[2 0 1 2 2 1 1 1 0 0 1 1 0 1 2 1 2 0 0 0 1 2 2 2 2 0 0 2 1 2 2 2 2 0 0 1 2\n",
            " 1 0 1 1 1 0 1 2 1 1 2 2 1 1 2 1 2 1 2 2 0 1 0]\n",
            "Accuracy: 0.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:272: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:272: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm3YTxzr1b74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b27d756c-9c06-4a32-bbfd-461df23ccf72"
      },
      "source": [
        "# data = datasets.load_iris()\n",
        "# X = data.data\n",
        "# y = data.target\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
        "\n",
        "# clf = XGBoostForest(n_estimators=50,learning_rate=0.05,max_depth=2,tree_lambda=0.0001)\n",
        "# clf.fit(X_train, y_train)\n",
        "# y_pred = clf.predict(X_test)\n",
        "# print(y_pred)\n",
        "# print(y_test)\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# print (\"Accuracy:\", accuracy)\n",
        "###################################################\n",
        "print(\"================Regression=================\")\n",
        "data = pd.read_csv('TempLinkoping2016.txt', sep=\"\\t\")\n",
        "\n",
        "time = np.atleast_2d(data[\"time\"].values).T\n",
        "temp = np.atleast_2d(data[\"temp\"].values).T\n",
        "\n",
        "X = time.reshape((-1, 1))               # Time. Fraction of the year [0, 1]\n",
        "X = np.insert(X, 0, values=1, axis=1)   # Insert bias term\n",
        "y = temp[:, 0]                          # Temperature. Reduce to one-dim\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
        "\n",
        "model = XGBoostForest(n_estimators=50,learning_rate=0.01,max_depth=4,tree_lambda=0.0001,classify=False)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_pred_line = model.predict(X)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print (\"Mean Squared Error:\", mse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\rTraining: N/A% [                                               ] ETA:  --:--:--"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "================Regression=================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Training: 100% [-----------------------------------------------] Time:  0:00:08\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 1.323393636282258e+16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfvNRYNunMaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaLQOnt5IhSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}