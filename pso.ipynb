{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pso.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMBWSwT7jv1iMeUCZrOlGRq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlowSight/MlAlgoFromScratch/blob/master/pso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrI2qYoEsn0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import copy\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RytLJjFudJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Loss(object):\n",
        "    def loss(self, y_true, y_pred):\n",
        "        return NotImplementedError()\n",
        "\n",
        "    def gradient(self, y, y_pred):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def acc(self, y, y_pred):\n",
        "        return 0\n",
        "\n",
        "class SquareLoss(Loss):\n",
        "    def __init__(self): pass\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        return 0.5 * np.power((y - y_pred), 2)\n",
        "\n",
        "    def gradient(self, y, y_pred):\n",
        "        return -(y - y_pred)\n",
        "\n",
        "    def hessian(self, y, y_pred):\n",
        "        return 1\n",
        "\n",
        "class CrossEntropy(Loss):\n",
        "    def __init__(self): pass\n",
        "\n",
        "    def loss(self, y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
        "\n",
        "    def acc(self, y, p):\n",
        "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
        "\n",
        "    def gradient(self, y, p):\n",
        "        # Avoid division by zero\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return - (y / p) + (1 - y) / (1 - p)\n",
        "\n",
        "    def hessian(self,y,p):\n",
        "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
        "        return y/(p**2) + (1-y)/(1-p)**2\n",
        "\n",
        "class LogisticLoss():\n",
        "    def __init__(self):\n",
        "        sigmoid = Sigmoid()\n",
        "        self.log_func = sigmoid\n",
        "        self.log_grad = sigmoid.gradient\n",
        "\n",
        "    def loss(self, y, y_pred):\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        p = self.log_func(y_pred)\n",
        "        return y * np.log(p) + (1 - y) * np.log(1 - p)\n",
        "  \n",
        "    def gradient(self, y, y_pred):\n",
        "        p = self.log_func(y_pred)\n",
        "        return -(y - p)\n",
        "\n",
        "    def hessian(self, y, y_pred):\n",
        "        p = self.log_func(y_pred)\n",
        "        return p * (1 - p)\n",
        "\n",
        "    def acc(self, y, y_pred):\n",
        "        p = self.log_func(y_pred)\n",
        "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
        "\n",
        "class TanH():\n",
        "    def __call__(self, x):\n",
        "        return 2 / (1 + np.exp(-2*x)) - 1\n",
        "    def gradient(self, x):\n",
        "        return 1 - np.power(self.__call__(x), 2)\n",
        "\n",
        "class ReLU():\n",
        "    def __call__(self, x):\n",
        "        return np.where(x >= 0, x, 0)\n",
        "    def gradient(self, x):\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "class Softmax():\n",
        "    def __call__(self, x):\n",
        "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "    def gradient(self, x):\n",
        "        p = self.__call__(x)\n",
        "        return p * (1 - p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyMZ7zQcEJih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer(object):\n",
        "\n",
        "    def set_input_shape(self, shape):\n",
        "        self.input_shape = shape\n",
        "\n",
        "    def layer_name(self):\n",
        "        return self.__class__.__name__\n",
        "\n",
        "    def parameters(self):\n",
        "        return 0\n",
        "\n",
        "    def forward_pass(self, X, training):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def output_shape(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, n_units, input_shape=None):\n",
        "        self.layer_input = None\n",
        "        self.input_shape = input_shape\n",
        "        self.n_units = n_units\n",
        "        self.trainable = True\n",
        "        self.W = None\n",
        "        self.w0 = None\n",
        "\n",
        "    def initialize(self, optimizer):\n",
        "        # Initialize the weights\n",
        "        limit = 1 / math.sqrt(self.input_shape[0])\n",
        "        self.W  = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n",
        "        self.w0 = np.zeros((1, self.n_units))\n",
        "        # Weight optimizers\n",
        "        self.W_opt  = copy.copy(optimizer)\n",
        "        self.w0_opt = copy.copy(optimizer)\n",
        "\n",
        "    def parameters(self):\n",
        "        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n",
        "\n",
        "    def forward_pass(self, X, training=True):\n",
        "        self.layer_input = X\n",
        "        return X.dot(self.W) + self.w0\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        W = self.W\n",
        "        if self.trainable:\n",
        "            grad_w = self.layer_input.T.dot(accum_grad)\n",
        "            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n",
        "\n",
        "            # Update the layer weights\n",
        "            self.W = self.W_opt.update(self.W, grad_w)\n",
        "            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n",
        "\n",
        "        accum_grad = accum_grad.dot(W.T)\n",
        "        return accum_grad\n",
        "\n",
        "    def output_shape(self):\n",
        "        return (self.n_units, )\n",
        "\n",
        "class Activation(Layer):\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.activation_name = name\n",
        "        self.activation_func = activation_functions[name]()\n",
        "        self.trainable = True\n",
        "\n",
        "    def layer_name(self):\n",
        "        return \"Activation (%s)\" % (self.activation_func.__class__.__name__)\n",
        "\n",
        "    def forward_pass(self, X, training=True):\n",
        "        self.layer_input = X\n",
        "        return self.activation_func(X)\n",
        "\n",
        "    def backward_pass(self, accum_grad):\n",
        "        return accum_grad * self.activation_func.gradient(self.layer_input)\n",
        "\n",
        "    def output_shape(self):\n",
        "        return self.input_shape\n",
        "\n",
        "class Adam():\n",
        "    def __init__(self, learning_rate=0.001, b1=0.9, b2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.eps = 1e-8\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        # Decay rates\n",
        "        self.b1 = b1\n",
        "        self.b2 = b2\n",
        "\n",
        "    def update(self, w, grad_wrt_w):\n",
        "        # If not initialized\n",
        "        if self.m is None:\n",
        "            self.m = np.zeros(np.shape(grad_wrt_w))\n",
        "            self.v = np.zeros(np.shape(grad_wrt_w))\n",
        "        \n",
        "        self.m = self.b1 * self.m + (1 - self.b1) * grad_wrt_w\n",
        "        self.v = self.b2 * self.v + (1 - self.b2) * np.power(grad_wrt_w, 2)\n",
        "\n",
        "        m_hat = self.m / (1 - self.b1)\n",
        "        v_hat = self.v / (1 - self.b2)\n",
        "\n",
        "        self.w_updt = self.learning_rate * m_hat / (np.sqrt(v_hat) + self.eps)\n",
        "\n",
        "        return w - self.w_updt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVQpPSorr_iL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ParticleSwarmOptimizedNN():\n",
        "    \"\"\" Particle Swarm Optimization of Neural Network.\n",
        "        https://visualstudiomagazine.com/articles/2013/12/01/neural-network-training-using-particle-swarm-optimization.aspx \n",
        "    \"\"\"\n",
        "    def __init__(self, population_size, \n",
        "                        model_builder, \n",
        "                        inertia_weight=0.8, \n",
        "                        cognitive_weight=2, \n",
        "                        social_weight=2, \n",
        "                        max_velocity=20):\n",
        "        self.population_size = population_size\n",
        "        self.model_builder = model_builder\n",
        "        self.best_individual = None\n",
        "        # Parameters used to update velocity\n",
        "        self.cognitive_w = cognitive_weight\n",
        "        self.inertia_w = inertia_weight\n",
        "        self.social_w = social_weight\n",
        "        self.min_v = -max_velocity\n",
        "        self.max_v = max_velocity\n",
        "\n",
        "    def _build_model(self, id):\n",
        "        \"\"\" Returns a new individual \"\"\"\n",
        "        model = self.model_builder(n_inputs=self.X.shape[1], n_outputs=self.y.shape[1])\n",
        "        model.id = id\n",
        "        model.fitness = 0\n",
        "        model.highest_fitness = 0\n",
        "        model.accuracy = 0\n",
        "        # Set intial best as the current initialization\n",
        "        model.best_layers = copy.copy(model.layers)\n",
        "\n",
        "        # Set initial velocity to zero\n",
        "        model.velocity = []\n",
        "        for layer in model.layers:\n",
        "            velocity = {\"W\": 0, \"w0\": 0}\n",
        "            if hasattr(layer, 'W'):\n",
        "                velocity = {\"W\": np.zeros_like(layer.W), \"w0\": np.zeros_like(layer.w0)}\n",
        "            model.velocity.append(velocity)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _initialize_population(self):\n",
        "        \"\"\" Initialization of the neural networks forming the population\"\"\"\n",
        "        self.population = []\n",
        "        for i in range(self.population_size):\n",
        "            model = self._build_model(id=i)\n",
        "            self.population.append(model)\n",
        "\n",
        "    def _update_weights(self, individual):\n",
        "        \"\"\" Calculate the new velocity and update weights for each layer \"\"\"\n",
        "        # Two random parameters used to update the velocity\n",
        "        r1 = np.random.uniform()\n",
        "        r2 = np.random.uniform()\n",
        "        for i, layer in enumerate(individual.layers):\n",
        "            if hasattr(layer, 'W'):\n",
        "                # Layer weights velocity\n",
        "                first_term_W = self.inertia_w * individual.velocity[i][\"W\"]\n",
        "                second_term_W = self.cognitive_w * r1 * (individual.best_layers[i].W - layer.W)\n",
        "                third_term_W = self.social_w * r2 * (self.best_individual.layers[i].W - layer.W)\n",
        "                new_velocity = first_term_W + second_term_W + third_term_W\n",
        "                individual.velocity[i][\"W\"] = np.clip(new_velocity, self.min_v, self.max_v)\n",
        "\n",
        "                # Bias weight velocity\n",
        "                first_term_w0 = self.inertia_w * individual.velocity[i][\"w0\"]\n",
        "                second_term_w0 = self.cognitive_w * r1 * (individual.best_layers[i].w0 - layer.w0)\n",
        "                third_term_w0 = self.social_w * r2 * (self.best_individual.layers[i].w0 - layer.w0)\n",
        "                new_velocity = first_term_w0 + second_term_w0 + third_term_w0\n",
        "                individual.velocity[i][\"w0\"] = np.clip(new_velocity, self.min_v, self.max_v)\n",
        "\n",
        "                # Update layer weights with velocity\n",
        "                individual.layers[i].W += individual.velocity[i][\"W\"]\n",
        "                individual.layers[i].w0 += individual.velocity[i][\"w0\"]\n",
        "        \n",
        "    def _calculate_fitness(self, individual):\n",
        "        \"\"\" Evaluate the individual on the test set to get fitness scores \"\"\"\n",
        "        loss, acc = individual.predict(self.X, self.y)\n",
        "        individual.fitness = 1 / (loss + 1e-8)\n",
        "        individual.accuracy = acc\n",
        "\n",
        "    def fit(self, X, y, n_generations):\n",
        "        \"\"\" Will evolve the population for n_generations based on dataset X and labels y\"\"\"\n",
        "        self.X, self.y = X, y\n",
        "\n",
        "        self._initialize_population()\n",
        "\n",
        "        # The best individual of the population is initialized as population's first ind.\n",
        "        self.best_individual = copy.copy(self.population[0])\n",
        "\n",
        "        for epoch in range(n_generations):\n",
        "            for individual in self.population:\n",
        "                self._update_weights(individual)\n",
        "                self._calculate_fitness(individual)\n",
        "\n",
        "                # If the current fitness is higher than the individual's previous highest\n",
        "                # => update the individual's best layer setup\n",
        "                if individual.fitness > individual.highest_fitness:\n",
        "                    individual.best_layers = copy.copy(individual.layers)\n",
        "                    individual.highest_fitness = individual.fitness\n",
        "                # If the individual's fitness is higher than the highest recorded fitness for the\n",
        "                # whole population => update the best individual\n",
        "                if individual.fitness > self.best_individual.fitness:\n",
        "                    self.best_individual = copy.copy(individual)\n",
        "\n",
        "            print (\"[%d Best Individual - ID: %d Fitness: %.5f, Accuracy: %.1f%%]\" % (epoch,\n",
        "                                                                            self.best_individual.id,\n",
        "                                                                            self.best_individual.fitness,\n",
        "                                                                            100*float(self.best_individual.accuracy)))\n",
        "        return self.best_individual\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self, optimizer, loss, validation_data=None):\n",
        "        self.optimizer = optimizer\n",
        "        self.layers = []\n",
        "        self.errors = {\"training\": [], \"validation\": []}\n",
        "        self.loss_function = loss()\n",
        "        self.progressbar = progressbar.ProgressBar(widgets= [\n",
        "                    'Training: ', progressbar.Percentage(), ' ', progressbar.Bar(marker=\"-\", left=\"[\", right=\"]\"),\n",
        "                    ' ', progressbar.ETA()])\n",
        "        self.val_set = None\n",
        "        if validation_data:\n",
        "            X, y = validation_data\n",
        "            self.val_set = {\"X\": X, \"y\": y}\n",
        "\n",
        "    def set_trainable(self, trainable):\n",
        "        for layer in self.layers:\n",
        "            layer.trainable = trainable\n",
        "\n",
        "    def add(self, layer):\n",
        "        if self.layers:\n",
        "            layer.set_input_shape(shape=self.layers[-1].output_shape())\n",
        "\n",
        "        if hasattr(layer, 'initialize'):\n",
        "            layer.initialize(optimizer=self.optimizer)\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def predict(self, X, y):\n",
        "        y_pred = self._forward_pass(X, training=False)\n",
        "        loss = np.mean(self.loss_function.loss(y, y_pred))\n",
        "        acc = self.loss_function.acc(y, y_pred)\n",
        "\n",
        "        return loss, acc\n",
        "\n",
        "    def train_on_batch(self, X, y):\n",
        "        y_pred = self._forward_pass(X)\n",
        "        loss = np.mean(self.loss_function.loss(y, y_pred))\n",
        "        acc = self.loss_function.acc(y, y_pred)\n",
        "        loss_grad = self.loss_function.gradient(y, y_pred)\n",
        "        self._backward_pass(loss_grad=loss_grad)\n",
        "        return loss, acc\n",
        "\n",
        "    def fit(self, X, y, n_epochs, batch_size):\n",
        "        for _ in self.progressbar(range(n_epochs)):\n",
        "            batch_error = []\n",
        "            for X_batch, y_batch in batch_iterator(X, y, batch_size=batch_size):\n",
        "                loss, _ = self.train_on_batch(X_batch, y_batch)\n",
        "                batch_error.append(loss)\n",
        "            self.errors[\"training\"].append(np.mean(batch_error))\n",
        "            if self.val_set is not None:\n",
        "                val_loss, _ = self.predict(self.val_set[\"X\"], self.val_set[\"y\"])\n",
        "                self.errors[\"validation\"].append(val_loss)\n",
        "\n",
        "        return self.errors[\"training\"], self.errors[\"validation\"]\n",
        "\n",
        "    def _forward_pass(self, X, training=True):\n",
        "        layer_output = X\n",
        "        for layer in self.layers:\n",
        "            layer_output = layer.forward_pass(layer_output, training)\n",
        "        return layer_output\n",
        "\n",
        "    def _backward_pass(self, loss_grad):\n",
        "        for layer in reversed(self.layers):\n",
        "            loss_grad = layer.backward_pass(loss_grad)\n",
        "\n",
        "    def summary(self, name=\"Model Summary\"):\n",
        "        print (AsciiTable([[name]]).table)\n",
        "        print (\"Input Shape: %s\" % str(self.layers[0].input_shape))\n",
        "        table_data = [[\"Layer Type\", \"Parameters\", \"Output Shape\"]]\n",
        "        tot_params = 0\n",
        "        for layer in self.layers:\n",
        "            layer_name = layer.layer_name()\n",
        "            params = layer.parameters()\n",
        "            out_shape = layer.output_shape()\n",
        "            table_data.append([layer_name, str(params), str(out_shape)])\n",
        "            tot_params += params\n",
        "        print (AsciiTable(table_data).table)\n",
        "        print (\"Total Parameters: %d\\n\" % tot_params)\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        return self._forward_pass(X, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws-4WiuFuazs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = datasets.make_classification(n_samples=1000, n_features=10, n_classes=4, n_clusters_per_class=1, n_informative=2)\n",
        "\n",
        "data = datasets.load_iris()\n",
        "X = normalize(data.data)\n",
        "y = data.target\n",
        "y = to_categorical(y.astype(\"int\"))\n",
        "\n",
        "# Model builder\n",
        "def model_builder(n_inputs, n_outputs):    \n",
        "    model = NeuralNetwork(optimizer=Adam(), loss=CrossEntropy)\n",
        "    model.add(Dense(16, input_shape=(n_inputs,)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(n_outputs))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Print the model summary of a individual in the population\n",
        "print (\"\")\n",
        "model_builder(n_inputs=X.shape[1], n_outputs=y.shape[1]).summary()\n",
        "\n",
        "population_size = 100\n",
        "n_generations = 10\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, seed=1)\n",
        "\n",
        "inertia_weight = 0.8\n",
        "cognitive_weight = 0.8\n",
        "social_weight = 0.8\n",
        "\n",
        "print (\"Population Size: %d\" % population_size)\n",
        "print (\"Generations: %d\" % n_generations)\n",
        "print (\"\")\n",
        "print (\"Inertia Weight: %.2f\" % inertia_weight)\n",
        "print (\"Cognitive Weight: %.2f\" % cognitive_weight)\n",
        "print (\"Social Weight: %.2f\" % social_weight)\n",
        "print (\"\")\n",
        "\n",
        "model = ParticleSwarmOptimizedNN(population_size=population_size, \n",
        "                    inertia_weight=inertia_weight,\n",
        "                    cognitive_weight=cognitive_weight,\n",
        "                    social_weight=social_weight,\n",
        "                    max_velocity=5,\n",
        "                    model_builder=model_builder)\n",
        "\n",
        "model = model.fit(X_train, y_train, n_generations=n_generations)\n",
        "\n",
        "loss, accuracy = model.predict(X_test, y_test)\n",
        "\n",
        "print (\"Accuracy: %.1f%%\" % float(100*accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}