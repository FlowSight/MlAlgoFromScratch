{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regression_understanding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOnueLbltwWLlypIVUI3Dt+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlowSight/MlAlgoFromScratch/blob/master/regression_understanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvjt8FpoMeI0",
        "colab_type": "text"
      },
      "source": [
        "We need a regression-useable dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z7Fu8KNIJ_6A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c15b1184-55fd-4bb5-ef2a-04951e82e3e8"
      },
      "source": [
        "# gdrive_mount_path='/content/gdrive'\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUpKs0xgN7yp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "60efa189-1cae-4704-ad67-852ab6ae0504"
      },
      "source": [
        "#%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/practices\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00246/3D_spatial_network.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-23 17:36:38--  https://archive.ics.uci.edu/ml/machine-learning-databases/00246/3D_spatial_network.txt\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20673913 (20M) [application/x-httpd-php]\n",
            "Saving to: ‘3D_spatial_network.txt’\n",
            "\n",
            "3D_spatial_network. 100%[===================>]  19.72M  11.3MB/s    in 1.7s    \n",
            "\n",
            "2020-02-23 17:36:40 (11.3 MB/s) - ‘3D_spatial_network.txt’ saved [20673913/20673913]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WPHpvtOGk-a1",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "dataset = pd.read_csv(\"3D_spatial_network.txt\", sep=\",\", header=None)\n",
        "from sklearn.utils import shuffle\n",
        "dataset = shuffle(dataset)\n",
        "X=dataset.iloc[:,1:3].to_numpy()\n",
        "Y=dataset.iloc[:,-1].to_numpy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
        "X_train=np.array(X_train).reshape((X_train.shape[0],X_train.shape[1]))\n",
        "y_train=np.array(y_train).reshape((y_train.shape[0],1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMZEat-LQio1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # This would do linear and ridge regression conditionally\n",
        " class LinearRegressionCustom:\n",
        "\n",
        "  def __init__(self,x,y, epochs=10000,learning_rate=0.0001,ridge=False,ridgeCoef=1.0,printEpochInterval=100):  \n",
        "        self.x = x\n",
        "        self.y=y\n",
        "        self.n_train=self.x.shape[0]\n",
        "        self.m=self.x.shape[1]\n",
        "        self.epochs=epochs\n",
        "        self.printEpochInterval=printEpochInterval\n",
        "        assert(printEpochInterval>=5)\n",
        "        self.learning_rate=learning_rate\n",
        "        self.ridge=ridge\n",
        "        self.ridgeCoef=ridgeCoef\n",
        "        if ridge==False:\n",
        "          self.ridgeCoef=0.0\n",
        "        self.bias=0.0\n",
        "        self.weights=np.full((self.m, 1), 1.0)\n",
        "        self.best_weights=np.full((self.m, 1), 1.0)\n",
        "        self.best_error=2.0**30\n",
        "        assert(y.shape[1]==1)\n",
        "        assert(x.shape[0]==y.shape[0])\n",
        "        self.errors = np.zeros(self.epochs)\n",
        "        self.LinearRegression()\n",
        "\n",
        "  def LSEGradDes(self):\n",
        "   # initialize stuff\n",
        "   error=0.0\n",
        "   gradients=np.full((self.m, 1), 0.0)\n",
        "   bias_grad=0\n",
        "   \n",
        "   y_pred = np.add(self.bias,np.dot(self.x,self.weights))\n",
        "   error=np.square(np.subtract(self.y, y_pred)).mean(axis=0) # SE error\n",
        "   gradients = -2/self.n_train*(self.x.T.dot(np.subtract(self.y,y_pred))+self.ridgeCoef*self.best_weights)  # Derivative wrt weights\n",
        "   bias_grad = -2/self.n_train*np.sum(np.subtract(self.y, y_pred),axis=0)  # Derivative wrt bias\n",
        "   return gradients,bias_grad,error\n",
        "  \n",
        "  def LinearRegression(self):\n",
        "    # start action in epochs\n",
        "    for epoch in range(self.epochs):\n",
        "      gradients,bias_grad,epoch_loss = [],0.0,0.0\n",
        "      gradients,bias_grad,epoch_loss=self.LSEGradDes()\n",
        "      \n",
        "      # update weights\n",
        "      best_error=2.0**30\n",
        "      self.bias = self.bias - self.learning_rate * bias_grad#bias update\n",
        "      self.weights = np.subtract(self.weights,self.learning_rate * gradients)\n",
        "      self.errors[epoch]=epoch_loss\n",
        "      if epoch_loss < np.amin(self.errors):\n",
        "        self.best_weights=self.best_weights\n",
        "        self.best_error=epoch_loss\n",
        "      if epoch%self.printEpochInterval==0 :\n",
        "        print(\"epoch: \"+str(epoch+1))\n",
        "        print(\"weights: \"+str(self.weights))\n",
        "        print(\"bias: \"+str(self.bias))\n",
        "        print(\"loss: \"+str(epoch_loss))\n",
        "\n",
        "  # this will return result and mse error as a tuple\n",
        "  def predict(self,X_test):\n",
        "    y_pred = np.add(self.bias,np.dot(self.x,self.weights))\n",
        "    #error=np.square(np.subtract(y_test, y_pred)).mean(axis=0)\n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YMrLNNH0lhJ8",
        "colab": {}
      },
      "source": [
        "LinearRegressionCustom(X_train,y_train,learning_rate=0.0001,epochs=200,printEpochInterval=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDikok9C7493",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "from scipy import linalg\n",
        "import numpy as np\n",
        "from itertools import combinations_with_replacement\n",
        "\n",
        "\n",
        "class PolynomialRegressionCustom(object):\n",
        "\n",
        "    def __init__(self, x, y): \n",
        "        self.x = x\n",
        "        self.y = y    \n",
        "\n",
        "    def normalize(self,v):\n",
        "     return v / np.sqrt(v.dot(v))\n",
        "\n",
        "    def gramSchmidt(self,X):\n",
        "      n = X.shape[1]\n",
        "      X[:, 0] = self.normalize(X[:, 0])\n",
        "      for i in range(1, n):\n",
        "        print(i)\n",
        "        Xi = X[:, i]\n",
        "        for j in range(0, i):\n",
        "            Xj = X[:, j]\n",
        "            Xi = Xi - (Xi.dot(Xj))* Xj\n",
        "        X[:, i] = self.normalize(Xi)\n",
        "      return X\n",
        "\n",
        "    def index_combinations(self):\n",
        "        combinations = [combinations_with_replacement(range(self.n_features), i) for i in range(0, self.order + 1)]\n",
        "        flat_combs = [item for sublist in combinations for item in sublist]\n",
        "        return flat_combs\n",
        "\n",
        "    def polynomial_transform(self,X, degree):\n",
        "        n_samples, self.n_features = np.shape(X)\n",
        "        combinations = self.index_combinations()\n",
        "        n_output_features = len(combinations)\n",
        "        X_new = np.empty((n_samples, n_output_features))\n",
        "        \n",
        "        for i, index_combs in enumerate(combinations):  \n",
        "            X_new[:, i] = np.prod(X[:, index_combs], axis=1)\n",
        "\n",
        "        return X_new       \n",
        "        \n",
        "    def fit(self, method = 'ne', order = 1,epochs = 60, learningRate = 0.01,printEpochInterval=100,ridge=False):\n",
        "        theta=0\n",
        "        self.order=order\n",
        "        self.method=method\n",
        "        tranformed_x =self.gramSchmidt(self.polynomial_transform(self.x,self.order))\n",
        "        if method == 'ne': \n",
        "            theta = np.matmul(np.matmul(linalg.pinv(np.matmul(np.transpose(tranformed_x),tranformed_x)), np.transpose(tranformed_x)), self.y)\n",
        "\n",
        "        elif method == 'gd': \n",
        "            lr = LinearRegressionCustom(tranformed_x,self.y,learning_rate=learningRate,epochs=epochs,printEpochInterval=printEpochInterval,ridge=ridge)\n",
        "            self.model=lr \n",
        "        self.theta = theta\n",
        "\n",
        "    def predict(self,X_test):\n",
        "       tranformed_x =self.gramSchmidt(self.polynomial_transform(X_test,self.order))\n",
        "       if self.method == 'ne':\n",
        "         return self.theta*X_test\n",
        "\n",
        "       elif self.method == 'gd':\n",
        "         return self.model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iCSavPoCaI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, print_function\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def normal_density(x,mean,var,eps=1e-4):\n",
        "   return (1.0 / math.sqrt(2.0 * math.pi * var + eps))* math.exp(-(math.pow(x - mean, 2)/ (2 * var + eps)))\n",
        "\n",
        "class NaiveBayes():\n",
        "    def fit(self, X, y):\n",
        "        self.X, self.y = X, y\n",
        "        self.classes = np.unique(y)\n",
        "        assert(self.classes <= 10)\n",
        "        self.parameters = []\n",
        "        for i, c in enumerate(self.classes):\n",
        "            X_where_c = X[np.where(y == c)]\n",
        "            self.parameters.append([])\n",
        "            colidx=0\n",
        "            for col in X_where_c.T:\n",
        "                parameters = {\"mean\"+str(colidx): col.mean(), \"var\"+str(colidx): col.var()}\n",
        "                self.parameters[i].append(parameters)\n",
        "                colidx+=1\n",
        "\n",
        "    def likelihood(self,mean, var, x,kernel=\"gaussian\"):\n",
        "        assert(kernel==\"gaussian\")\n",
        "        if kernel==\"gaussian\":\n",
        "          return normal_density(x,mean,var)\n",
        "\n",
        "    def classify(self, sample):\n",
        "        posteriors = []\n",
        "        for i, c in enumerate(self.classes):\n",
        "            prior = np.mean(self.y == c)\n",
        "            # p(y|x) = p(y)*p(x1|y)*p(x2|y)*p(x3|y)...\n",
        "            posterior = prior\n",
        "            for feature_value, params in zip(sample, self.parameters[i]):\n",
        "                likelihood = self.likelihood(params[\"mean\"], params[\"var\"], feature_value)\n",
        "                posterior *= likelihood\n",
        "            posteriors.append(posterior)\n",
        "        return self.classes[np.argmax(posteriors)]\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = [self.classify(sample) for sample in X]\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4kS5sEkHZWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_kernel(x1, x2):\n",
        "      return np.dot(x1, x2)\n",
        "\n",
        "def polynomial_kernel(x, y, p=3):\n",
        "      return (1 + np.dot(x, y)) ** p\n",
        "\n",
        "def gaussian_kernel(x, y, sigma=5.0):\n",
        "      return np.exp(-np.linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n",
        "\n",
        "def rbf_kernel(x1, x2,gamma=0.1):\n",
        "     distance = np.linalg.norm(x1 - x2) ** 2\n",
        "     return np.exp(-gamma * distance)\n",
        "\n",
        "class SVM(object):\n",
        "    \n",
        "    def __init__(self, kernel=linear_kernel, C=None,p=None,gamma=None,sigma=None,tol=1e-5):\n",
        "        self.kernel = kernel\n",
        "        self.C = C\n",
        "        self.tol=tol\n",
        "        self.p=p\n",
        "        self.gamma=gamma\n",
        "        self.sigma=sigma\n",
        "        if self.C is not None: self.C = float(self.C)\n",
        "        if self.p is not None: self.p = float(self.p)\n",
        "        if self.sigma is not None: self.sigma = float(self.sigma)\n",
        "        if self.gamma is not None: self.gamma = float(self.gamma)\n",
        "        if self.kernel is gaussian_kernel :\n",
        "          assert(self.sigma is not None)\n",
        "        if self.kernel is polynomial_kernel :\n",
        "          assert(self.p is not None)\n",
        "        if self.kernel is rbf_kernel :\n",
        "          assert(self.gamma is not None)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        #1) We will first get lagrangians\n",
        "        # 2) then we will have W, but we will calculate it runtime w*x=sum(lagrangian[i]*y[i]*<x[i],x[j]>)\n",
        "        # 3) we can get bias b = sum(yi-w*xi)/total_elements\n",
        "        K = np.zeros((n_samples, n_samples))\n",
        "        for i in range(n_samples):\n",
        "            for j in range(n_samples):\n",
        "              if self.kernel is polynomial_kernel:\n",
        "                K[i,j] = self.kernel(X[i], X[j],p=self.p)\n",
        "              elif self.kernel is gaussian_kernel:\n",
        "                K[i,j] = self.kernel(X[i], X[j],sigma=self.sigma)\n",
        "              elif self.kernel is rbf_kernel:\n",
        "                K[i,j] = self.kernel(X[i], X[j],gamma=self.gamma)\n",
        "              else:\n",
        "                K[i,j] = self.kernel(X[i], X[j])\n",
        "\n",
        "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
        "        q = cvxopt.matrix(np.ones(n_samples) * -1) # creating a nx1 -1 matrix\n",
        "        A = cvxopt.matrix(y, (1,n_samples)) # this is basically reshaping y into (1xn)\n",
        "        b = cvxopt.matrix(0.0)\n",
        "\n",
        "        if self.C is None:\n",
        "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
        "            h = cvxopt.matrix(np.zeros(n_samples))\n",
        "        else:\n",
        "            tmp1 = np.diag(np.ones(n_samples) * -1) # diagonal matrix of -1 sized nxn\n",
        "            tmp2 = np.identity(n_samples) # daigonal matrix of 1 sized nxn\n",
        "            G = cvxopt.matrix(np.vstack((tmp1, tmp2))) # stack tmp1 and tmp2 vertically\n",
        "            h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * self.C))) # horizontally stack tmp1 and tmp2\n",
        "\n",
        "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
        "        a = np.ravel(solution['x'])\n",
        "        assert(len(a)==X.shape[0])\n",
        "\n",
        "        # Support vectors have non zero lagrange multipliers\n",
        "        sv = a > self.tol\n",
        "        ind = np.arange(len(a))[sv]\n",
        "        print(ind)\n",
        "        self.a = a[sv]\n",
        "        self.sv = X[sv]\n",
        "        self.sv_y = y[sv]\n",
        "        print (str(len(self.a))+\" support vectors out of \"+str(n_samples) +\" points\")\n",
        "\n",
        "        # b=sum(yi-w*xi)/n\n",
        "        self.b = 0\n",
        "        for n in range(len(self.a)):\n",
        "            self.b += self.sv_y[n]\n",
        "            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
        "        self.b /= len(self.a)\n",
        "\n",
        "        # Weight vector\n",
        "        if self.kernel == linear_kernel:\n",
        "            self.w = np.zeros(n_features)\n",
        "            for n in range(len(self.a)):\n",
        "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
        "        else:\n",
        "            self.w = None\n",
        "\n",
        "    def project(self, X):\n",
        "        if self.w is not None:\n",
        "            y_predict = np.dot(X, self.w) + self.b\n",
        "        else:\n",
        "            y_predict = np.zeros(len(X))\n",
        "            for i in range(len(X)):\n",
        "                s = 0\n",
        "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
        "                    s += a * sv_y * self.kernel(X[i], sv)\n",
        "                y_predict[i] = s\n",
        "            y_predict = y_predict + self.b\n",
        "\n",
        "        return np.sign(y_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oxw4cx-17ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wph5h4MPbYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}